# Домашнее задание: Пишем Spark Structured Streaming Job

## Цель

В рамках данного задания студентам предлагается написать свою Spark Streaming Job на основе реального потока данных из Википедии.

Внимание!

Из необходимого для успешной работы всей инфраструктурной обвязки:
- docker
- maven (любой 3.X версии)
- Makefile (в противном случае команды можно запускать руками, копируя их из Makefile).

## Для решения данного задания необходимо:

1. Сделать форк от master из репозитория https://github.com/renardeinside/wikiflow/
2. Создать docker network командой ```make create-network```
3. Запустить стриминговую инфраструктуру командой ```make run-appliance```
4. Проверить, что в логах пишутся сообщения по типу:
```
producer_1 | 12:41:50.092 [default-akka.actor.default-dispatcher-23] INFO c.r.w.t.DataTransporterApp$ - New messages came, total: 48600 messages
```
5. Не закрывать и не останавливать процесс ```make-run-appliance``` (для дальнейшей разработки удобно держать его включенным). 
6. Перейти в объект ```AnalyticsConsumer``` в модуле ```com.renarde.wikiflow.consumer``` 
7. Имплементировать следующую логику работы с входными данными:
- из прилетающего стрима выбрать только ключи и значения, провести преобразование типов к кортежу (String,String)
- отфильтровать строки с пустыми значениями (```value.isNotNull```)
- провести преобразование входного json-объекта (которым и является value) в структуру через функцию from_json
- удалить всю активность ботов из входящего потока (```bot !=true```)
- сгруппировать по полю "type", посчитать каунты, добавить текущий timestamp и записать из в выходной стрим (объект ```transformedStream```. 
8. Запустить процесс командой ```make run-analytics-consumer```
9. Прислать форк в качестве решения ДЗ

### Advanced:

- попробуйте написать consumer-читатель из папки в которую пишет ```AnalyticsConsumer```, который просто будет выводить его на консоль. 
- попробуйте добавить sliding-window который агрегирует подсчет